"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö."""

import argparse
import json
from pathlib import Path
from typing import Any

import pandas as pd
import yaml
from sklearn.model_selection import train_test_split

from src.data_science_project.config_models import TrainingConfig

# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
RAW_DATA = Path("data/raw/WineQT.csv")
PROCESSED_DIR = Path("data/processed")
REPORTS_DIR = Path("reports")


def prepare_data(config_file: Path) -> None:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.

    Args:
        config_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    (REPORTS_DIR / "metrics").mkdir(parents=True, exist_ok=True)
    (REPORTS_DIR / "plots").mkdir(parents=True, exist_ok=True)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(config_file) as f:
        config_dict = yaml.safe_load(f)

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
    training_config_dict = {
        "data": config_dict["data"],
    }
    if "model" in config_dict:
        training_config_dict["model"] = config_dict["model"]

    training_config = TrainingConfig(**training_config_dict)

    data_config = training_config.data

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    df = pd.read_csv(RAW_DATA)

    # –ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –µ—Å–ª–∏ –µ—Å—Ç—å
    df = df.drop_duplicates()

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
    train_df, test_df = train_test_split(
        df,
        test_size=data_config.test_size,
        random_state=data_config.random_state,
        stratify=(
            df[data_config.target_column]
            if data_config.stratify and data_config.target_column in df.columns
            else None
        ),
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    train_df.to_csv(PROCESSED_DIR / "train.csv", index=False)
    test_df.to_csv(PROCESSED_DIR / "test.csv", index=False)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = {
        "train_size": len(train_df),
        "test_size": len(test_df),
        "total_size": len(df),
        "features": list(df.columns),
        "target": data_config.target_column,
    }

    with open(REPORTS_DIR / "metrics" / "data_stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    distribution_data: dict[str, Any]
    if data_config.target_column in df.columns:
        distribution = (
            df[data_config.target_column].value_counts().sort_index().to_dict()
        )
        distribution_data = {
            data_config.target_column: list(distribution.keys()),
            "count": list(distribution.values()),
        }
    else:
        distribution_data = {
            "message": f"Target column '{data_config.target_column}' not found"
        }

    with open(REPORTS_DIR / "plots" / "data_distribution.json", "w") as f:
        json.dump(distribution_data, f, indent=2)

    print("‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!")
    print(f"  Train: {len(train_df)} –∑–∞–ø–∏—Å–µ–π")
    print(f"  Test: {len(test_df)} –∑–∞–ø–∏—Å–µ–π")


def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    parser.add_argument("--config", type=str, default="config/train_params.yaml")
    args = parser.parse_args()

    config_file = Path(args.config)
    if not config_file.exists():
        raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_file}")

    prepare_data(config_file)


if __name__ == "__main__":
    main()
