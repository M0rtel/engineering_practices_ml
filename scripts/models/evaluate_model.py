"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏."""

import argparse
import json
import pickle  # nosec B403
from pathlib import Path

import pandas as pd
import yaml
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from src.data_science_project.config_models import TrainingConfig

# –ü—É—Ç–∏
MODEL_PATH = Path("models/model.pkl")
TEST_DATA = Path("data/processed/test.csv")
REPORTS_DIR = Path("reports")

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
(REPORTS_DIR / "metrics").mkdir(parents=True, exist_ok=True)
(REPORTS_DIR / "plots").mkdir(parents=True, exist_ok=True)


def evaluate_model(config_file: Path) -> None:
    """
    –û—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å.

    Args:
        config_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(config_file) as f:
        config_dict = yaml.safe_load(f)

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
    training_config_dict = {
        "data": config_dict["data"],
    }
    if "model" in config_dict:
        training_config_dict["model"] = config_dict["model"]

    training_config = TrainingConfig(**training_config_dict)

    data_config = training_config.data

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    with open(MODEL_PATH, "rb") as f:
        model = pickle.load(f)  # nosec B301

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_df = pd.read_csv(TEST_DATA)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    feature_cols = data_config.feature_columns
    target_col = data_config.target_column

    X_test = test_df[feature_cols]
    y_test = test_df[target_col]

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    y_pred = model.predict(X_test)

    # –ú–µ—Ç—Ä–∏–∫–∏
    metrics = {
        "test_mse": float(mean_squared_error(y_test, y_pred)),
        "test_rmse": float(mean_squared_error(y_test, y_pred) ** 0.5),
        "test_mae": float(mean_absolute_error(y_test, y_pred)),
        "test_r2": float(r2_score(y_test, y_pred)),
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    with open(REPORTS_DIR / "metrics" / "evaluation.json", "w") as f:
        json.dump(metrics, f, indent=2)

    # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è confusion matrix (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫)
    # –û–∫—Ä—É–≥–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ —Ü–µ–ª—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è "confusion matrix"
    y_pred_rounded = y_pred.round().astype(int)
    y_test_int = y_test.astype(int)

    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫)
    confusion_data = {
        "actual": y_test_int.tolist(),
        "predicted": y_pred_rounded.tolist(),
        "errors": (y_pred_rounded - y_test_int).tolist(),
    }

    with open(REPORTS_DIR / "plots" / "confusion_matrix.json", "w") as f:
        json.dump(confusion_data, f, indent=2)

    print("‚úÖ –ú–æ–¥–µ–ª—å –æ—Ü–µ–Ω–µ–Ω–∞!")
    print(f"  Test R¬≤: {metrics['test_r2']:.4f}")
    print(f"  Test RMSE: {metrics['test_rmse']:.4f}")


def main() -> None:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏")
    parser.add_argument("--config", type=str, default="config/train_params.yaml")
    args = parser.parse_args()

    config_file = Path(args.config)
    if not config_file.exists():
        raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_file}")

    evaluate_model(config_file)


if __name__ == "__main__":
    main()
